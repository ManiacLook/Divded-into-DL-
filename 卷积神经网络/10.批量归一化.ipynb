{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26d48fc9",
   "metadata": {},
   "source": [
    "标准化处理输入数据使各个特征的分布相近：这往往更容易训练出有效的模型。\n",
    "\n",
    "通常来说，数据标准化预处理对于浅层模型就足够有效了。随着模型训练的进行，当每层中参数更新时，靠近输出层的输出较难出现剧烈变化。但对深层神经网络来说，即使输入数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。这种计算数值的不稳定性通常令我们难以训练出有效的深度模型。\n",
    "\n",
    "批量归一化的提出正是为了应对深度模型训练的挑战。在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56866adb",
   "metadata": {},
   "source": [
    "# 批量归一化层\n",
    "对全连接层和卷积层做批量归一化的方法稍有不同。下面我们将分别介绍这两种情况下的批量归一化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c08f77",
   "metadata": {},
   "source": [
    "# 从零开始实现\n",
    "下面我们自己实现批量归一化层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0045f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../深度学习基础/\") \n",
    "import d2lzh_pytorch as d2l\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e72e150b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(is_training, X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    # 判断当前模式是训练模式还是预测模式\n",
    "    if not is_training:\n",
    "        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差\n",
    "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # 使用全连接层的情况，计算特征维上的均值和方差\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0)\n",
    "        else:\n",
    "            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。这里我们需要保持\n",
    "            # X的形状以便后面可以做广播运算\n",
    "            mean = X.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "            var = ((X - mean) ** 2).mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "        # 训练模式下用当前的均值和方差做标准化\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "        # 更新移动平均的均值和方差\n",
    "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "    Y = gamma * X_hat + beta  # 拉伸和偏移\n",
    "    return Y, moving_mean, moving_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1314b1c8",
   "metadata": {},
   "source": [
    "接下来，我们自定义一个`BatchNorm`层。它保存参与求梯度和迭代的拉伸参数`gamma`和偏移参数`beta`，同时也维护移动平均得到的均值和方差，以便能够在模型预测时被使用。`BatchNorm`实例所需指定的`num_features`参数对于全连接层来说应为输出个数，对于卷积层来说则为输出通道数。该实例所需指定的`num_dims`参数对于全连接层和卷积层来说分别为2和4。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dc44c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # 不参与求梯度和迭代的变量，全在内存上初始化成0\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.zeros(shape)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        # 保存更新过的moving_mean和moving_var, Module实例的traning属性默认为true, 调用.eval()后设成false\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(self.training, X, self.gamma, self.beta, \n",
    "                                                          self.moving_mean, self.moving_var, eps=1e-5, momentum=0.9)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5f8106",
   "metadata": {},
   "source": [
    "## 使用批量归一化层的LeNet\n",
    "下面我们修改前面介绍的`LeNet`模型，从而应用批量归一化层。我们在所有的卷积层或全连接层之后、激活层之前加入批量归一化层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bd0623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, 5), # in_channels, out_channels, kernel_size\n",
    "    BatchNorm(6, num_dims=4),\n",
    "    nn.Sigmoid(),\n",
    "    nn.MaxPool2d(2, 2), # kernel_size, stride\n",
    "    nn.Conv2d(6, 16, 5),\n",
    "    BatchNorm(16, num_dims=4),\n",
    "    nn.Sigmoid(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    d2l.FlattenLayer(),\n",
    "    nn.Linear(16*4*4, 120),\n",
    "    BatchNorm(120, num_dims=2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(120, 84),\n",
    "    BatchNorm(84, num_dims=2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(84, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d4b807",
   "metadata": {},
   "source": [
    "下面我们训练修改后的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "095fa19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 1.0032, train acc 0.790, test acc 0.788, time 8.6 sec\n",
      "epoch 2, loss 0.4544, train acc 0.867, test acc 0.809, time 5.2 sec\n",
      "epoch 3, loss 0.3653, train acc 0.880, test acc 0.859, time 5.5 sec\n",
      "epoch 4, loss 0.3282, train acc 0.889, test acc 0.871, time 5.5 sec\n",
      "epoch 5, loss 0.3077, train acc 0.893, test acc 0.858, time 5.3 sec\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)\n",
    "\n",
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40acd9bf",
   "metadata": {},
   "source": [
    "最后我们查看第一个批量归一化层学习到的拉伸参数`gamma`和偏移参数`beta`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16e22962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0201, 0.9996, 1.1485, 1.0359, 1.0107, 1.2626], device='cuda:0',\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor([-0.5166,  0.3237,  0.0827,  0.5576, -0.2722,  0.1798], device='cuda:0',\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[1].gamma.view((-1,)), net[1].beta.view((-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc60331",
   "metadata": {},
   "source": [
    "# 简洁实现\n",
    "与我们刚刚自己定义的`BatchNorm`类相比，Pytorch中`nn`模块定义的`BatchNorm1d`和`BatchNorm2d`类使用起来更加简单，二者分别用于全连接层和卷积层，都需要指定输入的`num_features`参数值。下面我们用PyTorch实现使用批量归一化的`LeNet`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f87b194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, 5),  # in_channels, out_channels, kernel_size\n",
    "    nn.BatchNorm2d(6),\n",
    "    nn.Sigmoid(),\n",
    "    nn.MaxPool2d(2, 2),  # kernel_size, stride\n",
    "    nn.Conv2d(6, 16, 5),\n",
    "    nn.BatchNorm2d(16),\n",
    "    nn.Sigmoid(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    d2l.FlattenLayer(),\n",
    "    nn.Linear(16 * 4 * 4, 120),\n",
    "    nn.BatchNorm1d(120),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(120, 84),\n",
    "    nn.BatchNorm1d(84),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(84, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57fbaf5",
   "metadata": {},
   "source": [
    "使用同样的超参数进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af945e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 0.9835, train acc 0.792, test acc 0.692, time 5.3 sec\n",
      "epoch 2, loss 0.4543, train acc 0.864, test acc 0.832, time 5.1 sec\n",
      "epoch 3, loss 0.3657, train acc 0.879, test acc 0.815, time 5.0 sec\n",
      "epoch 4, loss 0.3305, train acc 0.886, test acc 0.851, time 5.1 sec\n",
      "epoch 5, loss 0.3075, train acc 0.892, test acc 0.875, time 5.1 sec\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)\n",
    "\n",
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b4758a",
   "metadata": {},
   "source": [
    "# 小结\n",
    "+ 在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络的中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。\n",
    "+ 对全连接层和卷积层做批量归一化的方法稍有不同。\n",
    "+ 批量归一化层和丢弃层一样，在训练模式和预测模式的计算结果是不一样的。\n",
    "+ PyTorch提供了`BatchNorm`类方便使用。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
